{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlstm import xLSTM, Trainer\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.load('/home/intellect/Documents/Research/Current/ValleyForecast/data/cleaned/train.pt')\n",
    "val = torch.load('/home/intellect/Documents/Research/Current/ValleyForecast/data/cleaned/val.pt')\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SS layer\n",
    "# hs of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xLSTM(stack_config='ss', hidden_size=200)\n",
    "# model.to('cuda')\n",
    "# trainer = Trainer(model, 100, train_loader, val_loader)\n",
    "# start = time.time()\n",
    "# train_losses, val_losses = trainer.train()\n",
    "# end = time.time()\n",
    "\n",
    "# print(f'\\nTotal training time on 100 epochs: {end-start}')  \n",
    "# print(\"Train Losses: \")\n",
    "# print(train_losses)\n",
    "# print(\"#\"*100)\n",
    "# print(\"Val losses: \")\n",
    "# print(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS layer\n",
    "# hs of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xLSTM(stack_config='ms', hidden_size=200)\n",
    "# model.to('cuda')\n",
    "# trainer = Trainer(model, 100, train_loader, val_loader)\n",
    "# start = time.time()\n",
    "# train_losses, val_losses = trainer.train()\n",
    "# end = time.time()\n",
    "\n",
    "# print(f'\\nTotal training time on 100 epochs: {end-start}')  \n",
    "# print(\"Train Losses: \")\n",
    "# print(train_losses)\n",
    "# print(\"#\"*100)\n",
    "# print(\"Val losses: \")\n",
    "# print(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM layer \n",
    "# hs of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train RMSE Loss: 291.7238, Val RMSE Loss: 134.5314\n",
      "Epoch 2 Train RMSE Loss: 287.3270, Val RMSE Loss: 136.8419\n",
      "Epoch 3 Train RMSE Loss: 286.4044, Val RMSE Loss: 132.1306\n",
      "Epoch 4 Train RMSE Loss: 285.3604, Val RMSE Loss: 132.4424\n",
      "Epoch 5 Train RMSE Loss: 283.8715, Val RMSE Loss: 131.3296\n",
      "Epoch 6 Train RMSE Loss: 282.4876, Val RMSE Loss: 130.0794\n",
      "Epoch 7 Train RMSE Loss: 280.8166, Val RMSE Loss: 129.5631\n",
      "Epoch 8 Train RMSE Loss: 280.1078, Val RMSE Loss: 130.1653\n",
      "Epoch 9 Train RMSE Loss: 280.1691, Val RMSE Loss: 92.2696\n",
      "Epoch 10 Train RMSE Loss: 278.6673, Val RMSE Loss: 129.7076\n",
      "Epoch 11 Train RMSE Loss: 279.4744, Val RMSE Loss: 104.2697\n",
      "Epoch 12 Train RMSE Loss: 279.5224, Val RMSE Loss: 138.8411\n",
      "Epoch 13 Train RMSE Loss: 278.9876, Val RMSE Loss: 134.7963\n",
      "Epoch 14 Train RMSE Loss: 279.1558, Val RMSE Loss: 136.6143\n",
      "Epoch 15 Train RMSE Loss: 278.7652, Val RMSE Loss: 135.2175\n",
      "Epoch 16 Train RMSE Loss: 278.9576, Val RMSE Loss: 138.5643\n",
      "Epoch 17 Train RMSE Loss: 278.0576, Val RMSE Loss: 137.3897\n",
      "Epoch 18 Train RMSE Loss: 278.6541, Val RMSE Loss: 138.1705\n",
      "Epoch 19 Train RMSE Loss: 277.9720, Val RMSE Loss: 134.8764\n",
      "Epoch 20 Train RMSE Loss: 277.8136, Val RMSE Loss: 141.8937\n",
      "Epoch 21 Train RMSE Loss: 277.7807, Val RMSE Loss: 136.5608\n",
      "Epoch 22 Train RMSE Loss: 277.4904, Val RMSE Loss: 137.4912\n",
      "Epoch 23 Train RMSE Loss: 277.5316, Val RMSE Loss: 142.2421\n",
      "Epoch 24 Train RMSE Loss: 276.5594, Val RMSE Loss: 135.3462\n",
      "Epoch 25 Train RMSE Loss: 276.3492, Val RMSE Loss: 130.3691\n",
      "Epoch 26 Train RMSE Loss: 276.3632, Val RMSE Loss: 129.0114\n",
      "Epoch 27 Train RMSE Loss: 276.2521, Val RMSE Loss: 107.4493\n",
      "Epoch 28 Train RMSE Loss: 275.8652, Val RMSE Loss: 134.5731\n",
      "Epoch 29 Train RMSE Loss: 275.3354, Val RMSE Loss: 102.4698\n",
      "Epoch 30 Train RMSE Loss: 275.1543, Val RMSE Loss: 133.2633\n",
      "Epoch 31 Train RMSE Loss: 275.2094, Val RMSE Loss: 132.4641\n",
      "Epoch 32 Train RMSE Loss: 274.6370, Val RMSE Loss: 134.4718\n",
      "Epoch 33 Train RMSE Loss: 274.7012, Val RMSE Loss: 107.4663\n",
      "Epoch 34 Train RMSE Loss: 273.6202, Val RMSE Loss: 134.5002\n",
      "Epoch 35 Train RMSE Loss: 273.6588, Val RMSE Loss: 127.2613\n",
      "Epoch 36 Train RMSE Loss: 272.5494, Val RMSE Loss: 138.3586\n",
      "Epoch 37 Train RMSE Loss: 272.6546, Val RMSE Loss: 121.9306\n",
      "Epoch 38 Train RMSE Loss: 272.6585, Val RMSE Loss: 134.6012\n",
      "Epoch 39 Train RMSE Loss: 272.1218, Val RMSE Loss: 131.9450\n",
      "Epoch 40 Train RMSE Loss: 271.6282, Val RMSE Loss: 101.3520\n",
      "Epoch 41 Train RMSE Loss: 271.7042, Val RMSE Loss: 101.9870\n",
      "Epoch 42 Train RMSE Loss: 271.2322, Val RMSE Loss: 126.2084\n",
      "Epoch 43 Train RMSE Loss: 271.0149, Val RMSE Loss: 132.2838\n",
      "Epoch 44 Train RMSE Loss: 270.3323, Val RMSE Loss: 124.2161\n",
      "Epoch 45 Train RMSE Loss: 270.2119, Val RMSE Loss: 110.0230\n",
      "Epoch 46 Train RMSE Loss: 269.9077, Val RMSE Loss: 137.1629\n",
      "Epoch 47 Train RMSE Loss: 269.2078, Val RMSE Loss: 128.6000\n",
      "Epoch 48 Train RMSE Loss: 269.3959, Val RMSE Loss: 135.8179\n",
      "Epoch 49 Train RMSE Loss: 268.7818, Val RMSE Loss: 129.7050\n",
      "Epoch 50 Train RMSE Loss: 268.3025, Val RMSE Loss: 131.5487\n",
      "Epoch 51 Train RMSE Loss: 267.6765, Val RMSE Loss: 124.7720\n",
      "Epoch 52 Train RMSE Loss: 267.8579, Val RMSE Loss: 138.0212\n",
      "Epoch 53 Train RMSE Loss: 267.4069, Val RMSE Loss: 139.0340\n",
      "Epoch 54 Train RMSE Loss: 266.6637, Val RMSE Loss: 134.2821\n",
      "Epoch 55 Train RMSE Loss: 267.0952, Val RMSE Loss: 131.9789\n",
      "Epoch 56 Train RMSE Loss: 266.0771, Val RMSE Loss: 131.8215\n",
      "Epoch 57 Train RMSE Loss: 265.8931, Val RMSE Loss: 135.7452\n",
      "Epoch 58 Train RMSE Loss: 265.6309, Val RMSE Loss: 134.1423\n",
      "Epoch 59 Train RMSE Loss: 265.1778, Val RMSE Loss: 134.4934\n",
      "Epoch 60 Train RMSE Loss: 264.2821, Val RMSE Loss: 132.1628\n",
      "Epoch 61 Train RMSE Loss: 264.3481, Val RMSE Loss: 136.7174\n",
      "Epoch 62 Train RMSE Loss: 265.0315, Val RMSE Loss: 128.9957\n",
      "Epoch 63 Train RMSE Loss: 263.3820, Val RMSE Loss: 91.2290\n",
      "Epoch 64 Train RMSE Loss: 263.4926, Val RMSE Loss: 133.5908\n",
      "Epoch 65 Train RMSE Loss: 263.0423, Val RMSE Loss: 97.9456\n",
      "Epoch 66 Train RMSE Loss: 262.0698, Val RMSE Loss: 136.2332\n",
      "Epoch 67 Train RMSE Loss: 262.2436, Val RMSE Loss: 131.3303\n",
      "Epoch 68 Train RMSE Loss: 261.8903, Val RMSE Loss: 127.5151\n",
      "Epoch 69 Train RMSE Loss: 260.6516, Val RMSE Loss: 109.3671\n",
      "Epoch 70 Train RMSE Loss: 260.8254, Val RMSE Loss: 137.5188\n",
      "Epoch 71 Train RMSE Loss: 259.7454, Val RMSE Loss: 134.5267\n",
      "Epoch 72 Train RMSE Loss: 259.6380, Val RMSE Loss: 134.7153\n",
      "Epoch 73 Train RMSE Loss: 258.7023, Val RMSE Loss: 131.3005\n",
      "Epoch 74 Train RMSE Loss: 258.2863, Val RMSE Loss: 139.8589\n",
      "Epoch 75 Train RMSE Loss: 257.3278, Val RMSE Loss: 94.0201\n",
      "Epoch 76 Train RMSE Loss: 257.1875, Val RMSE Loss: 140.1880\n",
      "Epoch 77 Train RMSE Loss: 256.5745, Val RMSE Loss: 142.8753\n",
      "Epoch 78 Train RMSE Loss: 255.0888, Val RMSE Loss: 135.0602\n",
      "Epoch 79 Train RMSE Loss: 253.7755, Val RMSE Loss: 139.3902\n",
      "Epoch 80 Train RMSE Loss: 254.4022, Val RMSE Loss: 142.1280\n",
      "Epoch 81 Train RMSE Loss: 253.0143, Val RMSE Loss: 135.4912\n",
      "Epoch 82 Train RMSE Loss: 252.4691, Val RMSE Loss: 137.1352\n",
      "Epoch 83 Train RMSE Loss: 251.5262, Val RMSE Loss: 146.2278\n",
      "Epoch 84 Train RMSE Loss: 250.5263, Val RMSE Loss: 144.8883\n",
      "Epoch 85 Train RMSE Loss: 250.6120, Val RMSE Loss: 136.4187\n",
      "Epoch 86 Train RMSE Loss: 249.7597, Val RMSE Loss: 91.6917\n",
      "Epoch 87 Train RMSE Loss: 249.0668, Val RMSE Loss: 93.1125\n",
      "Epoch 88 Train RMSE Loss: 248.8791, Val RMSE Loss: 140.0954\n",
      "Epoch 89 Train RMSE Loss: 248.8730, Val RMSE Loss: 143.0594\n",
      "Epoch 90 Train RMSE Loss: 247.8957, Val RMSE Loss: 144.8954\n",
      "Epoch 91 Train RMSE Loss: 247.3284, Val RMSE Loss: 88.7660\n",
      "Epoch 92 Train RMSE Loss: 245.9749, Val RMSE Loss: 142.9736\n",
      "Epoch 93 Train RMSE Loss: 246.3290, Val RMSE Loss: 156.7217\n",
      "Epoch 94 Train RMSE Loss: 244.7624, Val RMSE Loss: 141.0776\n",
      "Epoch 95 Train RMSE Loss: 245.1014, Val RMSE Loss: 141.6232\n",
      "Epoch 96 Train RMSE Loss: 244.5826, Val RMSE Loss: 136.8669\n",
      "Epoch 97 Train RMSE Loss: 244.1062, Val RMSE Loss: 150.6495\n",
      "Epoch 98 Train RMSE Loss: 244.1968, Val RMSE Loss: 143.1572\n",
      "Epoch 99 Train RMSE Loss: 243.5215, Val RMSE Loss: 141.4621\n",
      "Epoch 100 Train RMSE Loss: 242.5517, Val RMSE Loss: 149.3024\n",
      "\n",
      "Total training time on 100 epochs: 34327.58019948006\n",
      "Train Losses: \n",
      "[291.72379459102984, 287.3270314087766, 286.40437339042467, 285.36041470003886, 283.87151439534915, 282.4875937285022, 280.8165798582702, 280.10780330385876, 280.16906621770124, 278.66729892659305, 279.4743566317705, 279.5223743174596, 278.9875562678884, 279.15580131554117, 278.76517912030266, 278.95763270478653, 278.0576228569291, 278.65407152578797, 277.9720424297306, 277.81360012696194, 277.7807086429017, 277.4904454896211, 277.53161908650196, 276.55943279873225, 276.3492135784306, 276.36324379550473, 276.2520901680943, 275.8652150602304, 275.33536303246126, 275.1542868419739, 275.20937466257783, 274.63700385468314, 274.70117778575644, 273.62019555480646, 273.6587694450018, 272.5493945631555, 272.6545720979657, 272.65852268129544, 272.121794305513, 271.6281603825728, 271.70417920303316, 271.2322463206848, 271.0148855763695, 270.3322968829003, 270.2118749984472, 269.9076529063002, 269.2078356633189, 269.39592489476536, 268.78177817710343, 268.30245286299055, 267.67654299880996, 267.85792954813087, 267.4069233314676, 266.66373752353405, 267.09521688593054, 266.0770573393484, 265.89310419834743, 265.63092436434596, 265.1778225801172, 264.2821102333937, 264.3481353197957, 265.03146294499504, 263.38197419663504, 263.49264730557246, 263.0422591324734, 262.06979616734213, 262.2436040451021, 261.8903300530483, 260.65163117716315, 260.8253659633897, 259.7453926411219, 259.63795747421545, 258.7023310053741, 258.28629415952116, 257.32780494519426, 257.18754229576837, 256.574450490261, 255.08884589699383, 253.77554431622227, 254.40223501727968, 253.01434208260932, 252.46907297784531, 251.52619436524722, 250.5263193635557, 250.6120231639192, 249.7596971362147, 249.06681948317765, 248.87911163579113, 248.8729606032079, 247.89572677065814, 247.32837584285954, 245.97486147587503, 246.32903709625734, 244.7623711034991, 245.1014191021206, 244.58261326884033, 244.10624335618922, 244.19680182165945, 243.52147295678853, 242.55173869451804]\n",
      "####################################################################################################\n",
      "Val losses: \n",
      "[134.53143691915005, 136.8419204105256, 132.13061626988414, 132.4424274242633, 131.32958524619463, 130.0793993931567, 129.5630841460541, 130.16532409470537, 92.2695807788752, 129.70759068778995, 104.26972610134736, 138.84106931775221, 134.79627640559104, 136.61426811264306, 135.217480259667, 138.56432184751782, 137.3897365062821, 138.17054714249434, 134.87644809231892, 141.89373585676933, 136.56080564237791, 137.49117381118165, 142.24211863052378, 135.34621855431766, 130.36905669054607, 129.01138421030672, 107.44925758953836, 134.57312746511536, 102.46983198622489, 133.26329652673266, 132.4640705265724, 134.47179017740348, 107.4663141035003, 134.50017516397747, 127.26132061858362, 138.35858536618264, 121.93057677120129, 134.60121765039403, 131.94497366030564, 101.35199940407256, 101.986985516995, 126.20839013020415, 132.28380431032554, 124.2160662052845, 110.02298232926155, 137.16292312599913, 128.59997374443867, 135.81794626030532, 129.70495129195507, 131.54870492550705, 124.77195849218911, 138.0211722676656, 139.03400866840548, 134.28211406299263, 131.97893055465923, 131.82151055321452, 135.74518184647016, 134.1422743965281, 134.49341666407528, 132.1628089011452, 136.7174357079685, 128.9957340924982, 91.22899145466828, 133.59076568304133, 97.94556020398207, 136.23318640882465, 131.33032001065695, 127.51510997322778, 109.36709737075721, 137.51877535165426, 134.5266844644876, 134.7153393502388, 131.30054179397214, 139.8589311766782, 94.02010438888523, 140.18800330103588, 142.87531398628556, 135.06018804418736, 139.39017596224332, 142.12798634313063, 135.4912277704146, 137.13520812547443, 146.2278053070683, 144.8883182173752, 136.4187257145658, 91.69167575805511, 93.11247884048792, 140.09541339214366, 143.05943930348715, 144.89542801135497, 88.76602105614865, 142.97361714734404, 156.72165220393526, 141.0776183849858, 141.62318821932462, 136.86694780467337, 150.6495303871186, 143.15716068800052, 141.46207159587092, 149.30243113353114]\n"
     ]
    }
   ],
   "source": [
    "# model = xLSTM(stack_config='sm', hidden_size=200)\n",
    "# model.to('cuda')\n",
    "# trainer = Trainer(model, 100, train_loader, val_loader)\n",
    "# start = time.time()\n",
    "# train_losses, val_losses = trainer.train()\n",
    "# end = time.time()\n",
    "\n",
    "# print(f'\\nTotal training time on 100 epochs: {end-start}')  \n",
    "# print(\"Train Losses: \")\n",
    "# print(train_losses)\n",
    "# print(\"#\"*100)\n",
    "# print(\"Val losses: \")\n",
    "# print(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ss layer 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train RMSE Loss: 290.9398, Val RMSE Loss: 91.4199\n",
      "Epoch 2 Train RMSE Loss: 287.1601, Val RMSE Loss: 136.7366\n",
      "Epoch 3 Train RMSE Loss: 285.9007, Val RMSE Loss: 138.1517\n",
      "Epoch 4 Train RMSE Loss: 283.6915, Val RMSE Loss: 93.6655\n",
      "Epoch 5 Train RMSE Loss: 283.0418, Val RMSE Loss: 135.5920\n",
      "Epoch 6 Train RMSE Loss: 281.9772, Val RMSE Loss: 139.4691\n",
      "Epoch 7 Train RMSE Loss: 279.9406, Val RMSE Loss: 139.9445\n",
      "Epoch 8 Train RMSE Loss: 276.7834, Val RMSE Loss: 97.6049\n",
      "Epoch 9 Train RMSE Loss: 274.1742, Val RMSE Loss: 129.3729\n",
      "Epoch 10 Train RMSE Loss: 268.3419, Val RMSE Loss: 136.7086\n",
      "Epoch 11 Train RMSE Loss: 261.3928, Val RMSE Loss: 131.3500\n",
      "Epoch 12 Train RMSE Loss: 252.3923, Val RMSE Loss: 131.3999\n",
      "Epoch 13 Train RMSE Loss: 243.6943, Val RMSE Loss: 134.9322\n",
      "Epoch 14 Train RMSE Loss: 235.3649, Val RMSE Loss: 120.9667\n",
      "Epoch 15 Train RMSE Loss: 227.7177, Val RMSE Loss: 71.0304\n",
      "Epoch 16 Train RMSE Loss: 221.4587, Val RMSE Loss: 116.4391\n",
      "Epoch 17 Train RMSE Loss: 217.3326, Val RMSE Loss: 117.5368\n",
      "Epoch 18 Train RMSE Loss: 212.7872, Val RMSE Loss: 119.8917\n",
      "Epoch 19 Train RMSE Loss: 210.3777, Val RMSE Loss: 120.8655\n",
      "Epoch 20 Train RMSE Loss: 207.4408, Val RMSE Loss: 119.4029\n",
      "Epoch 21 Train RMSE Loss: 205.1047, Val RMSE Loss: 113.8290\n",
      "Epoch 22 Train RMSE Loss: 203.6124, Val RMSE Loss: 120.1291\n",
      "Epoch 23 Train RMSE Loss: 200.3073, Val RMSE Loss: 117.2755\n",
      "Epoch 24 Train RMSE Loss: 198.6577, Val RMSE Loss: 111.3210\n",
      "Epoch 25 Train RMSE Loss: 195.9896, Val RMSE Loss: 116.5119\n",
      "Epoch 26 Train RMSE Loss: 194.1783, Val RMSE Loss: 117.7881\n",
      "Epoch 27 Train RMSE Loss: 192.1068, Val RMSE Loss: 107.3109\n",
      "Epoch 28 Train RMSE Loss: 191.2057, Val RMSE Loss: 121.7442\n",
      "Epoch 29 Train RMSE Loss: 186.7822, Val RMSE Loss: 115.7666\n",
      "Epoch 30 Train RMSE Loss: 184.8541, Val RMSE Loss: 115.7260\n",
      "Epoch 31 Train RMSE Loss: 184.8545, Val RMSE Loss: 120.7129\n",
      "Epoch 32 Train RMSE Loss: 181.8388, Val RMSE Loss: 119.5693\n",
      "Epoch 33 Train RMSE Loss: 178.8345, Val RMSE Loss: 96.3366\n",
      "Epoch 34 Train RMSE Loss: 178.1220, Val RMSE Loss: 76.0864\n",
      "Epoch 35 Train RMSE Loss: 176.0592, Val RMSE Loss: 114.5266\n",
      "Epoch 36 Train RMSE Loss: 174.1002, Val RMSE Loss: 119.2831\n",
      "Epoch 37 Train RMSE Loss: 172.9297, Val RMSE Loss: 106.7909\n",
      "Epoch 38 Train RMSE Loss: 170.9313, Val RMSE Loss: 116.6590\n",
      "Epoch 39 Train RMSE Loss: 168.3469, Val RMSE Loss: 112.0325\n",
      "Epoch 40 Train RMSE Loss: 166.4357, Val RMSE Loss: 88.8869\n",
      "Epoch 41 Train RMSE Loss: 165.4499, Val RMSE Loss: 118.3092\n",
      "Epoch 42 Train RMSE Loss: 162.5535, Val RMSE Loss: 114.6755\n",
      "Epoch 43 Train RMSE Loss: 162.4514, Val RMSE Loss: 124.1164\n",
      "Epoch 44 Train RMSE Loss: 160.6292, Val RMSE Loss: 112.1786\n",
      "Epoch 45 Train RMSE Loss: 159.0616, Val RMSE Loss: 116.0838\n",
      "Epoch 46 Train RMSE Loss: 156.9749, Val RMSE Loss: 119.3230\n",
      "Epoch 47 Train RMSE Loss: 155.4781, Val RMSE Loss: 137.8303\n",
      "Epoch 48 Train RMSE Loss: 154.9772, Val RMSE Loss: 100.4014\n",
      "Epoch 49 Train RMSE Loss: 152.9190, Val RMSE Loss: 123.7377\n",
      "Epoch 50 Train RMSE Loss: 151.0137, Val RMSE Loss: 117.9205\n",
      "Epoch 51 Train RMSE Loss: 149.5510, Val RMSE Loss: 124.1983\n",
      "Epoch 52 Train RMSE Loss: 149.6514, Val RMSE Loss: 115.8824\n",
      "Epoch 53 Train RMSE Loss: 146.2298, Val RMSE Loss: 119.0893\n",
      "Epoch 54 Train RMSE Loss: 145.0872, Val RMSE Loss: 146.2643\n",
      "Epoch 55 Train RMSE Loss: 144.7704, Val RMSE Loss: 129.2814\n",
      "Epoch 56 Train RMSE Loss: 142.5202, Val RMSE Loss: 119.2874\n",
      "Epoch 57 Train RMSE Loss: 143.0842, Val RMSE Loss: 118.9782\n",
      "Epoch 58 Train RMSE Loss: 141.1924, Val RMSE Loss: 135.5521\n",
      "Epoch 59 Train RMSE Loss: 140.8117, Val RMSE Loss: 130.0541\n",
      "Epoch 60 Train RMSE Loss: 139.3016, Val RMSE Loss: 141.5672\n",
      "Epoch 61 Train RMSE Loss: 138.8711, Val RMSE Loss: 117.1397\n",
      "Epoch 62 Train RMSE Loss: 137.4880, Val RMSE Loss: 120.1027\n",
      "Epoch 63 Train RMSE Loss: 138.0502, Val RMSE Loss: 137.1534\n",
      "Epoch 64 Train RMSE Loss: 136.1363, Val RMSE Loss: 153.6991\n",
      "Epoch 65 Train RMSE Loss: 133.7618, Val RMSE Loss: 97.4043\n",
      "Epoch 66 Train RMSE Loss: 135.7311, Val RMSE Loss: 135.0185\n",
      "Epoch 67 Train RMSE Loss: 134.8442, Val RMSE Loss: 131.9316\n",
      "Epoch 68 Train RMSE Loss: 134.9189, Val RMSE Loss: 124.9346\n",
      "Epoch 69 Train RMSE Loss: 132.1674, Val RMSE Loss: 155.9515\n",
      "Epoch 70 Train RMSE Loss: 132.2463, Val RMSE Loss: 141.8387\n",
      "Epoch 71 Train RMSE Loss: 130.6288, Val RMSE Loss: 106.7803\n",
      "Epoch 72 Train RMSE Loss: 131.2021, Val RMSE Loss: 113.2475\n",
      "Epoch 73 Train RMSE Loss: 129.1485, Val RMSE Loss: 137.2448\n",
      "Epoch 74 Train RMSE Loss: 129.6114, Val RMSE Loss: 152.8360\n",
      "Epoch 75 Train RMSE Loss: 128.1687, Val RMSE Loss: 117.9062\n",
      "Epoch 76 Train RMSE Loss: 127.7398, Val RMSE Loss: 149.5633\n",
      "Epoch 77 Train RMSE Loss: 127.2460, Val RMSE Loss: 140.7680\n",
      "Epoch 78 Train RMSE Loss: 124.6844, Val RMSE Loss: 121.5343\n",
      "Epoch 79 Train RMSE Loss: 125.5078, Val RMSE Loss: 144.1266\n",
      "Epoch 80 Train RMSE Loss: 126.4540, Val RMSE Loss: 132.8650\n",
      "Epoch 81 Train RMSE Loss: 126.2704, Val RMSE Loss: 127.0979\n",
      "Epoch 82 Train RMSE Loss: 125.7932, Val RMSE Loss: 141.3956\n",
      "Epoch 83 Train RMSE Loss: 126.1569, Val RMSE Loss: 114.7285\n",
      "Epoch 84 Train RMSE Loss: 125.0932, Val RMSE Loss: 151.3545\n",
      "Epoch 85 Train RMSE Loss: 124.3122, Val RMSE Loss: 126.0119\n",
      "Epoch 86 Train RMSE Loss: 123.9480, Val RMSE Loss: 190.1265\n",
      "Epoch 87 Train RMSE Loss: 123.6257, Val RMSE Loss: 137.0196\n",
      "Epoch 88 Train RMSE Loss: 122.2682, Val RMSE Loss: 118.4907\n",
      "Epoch 89 Train RMSE Loss: 122.1637, Val RMSE Loss: 125.2148\n",
      "Epoch 90 Train RMSE Loss: 121.1569, Val RMSE Loss: 168.5439\n",
      "Epoch 91 Train RMSE Loss: 120.5665, Val RMSE Loss: 126.5109\n",
      "Epoch 92 Train RMSE Loss: 120.6400, Val RMSE Loss: 130.0939\n",
      "Epoch 93 Train RMSE Loss: 120.8048, Val RMSE Loss: 154.1148\n",
      "Epoch 94 Train RMSE Loss: 119.2441, Val RMSE Loss: 128.9256\n",
      "Epoch 95 Train RMSE Loss: 118.8109, Val RMSE Loss: 136.6879\n",
      "Epoch 96 Train RMSE Loss: 120.8906, Val RMSE Loss: 144.3737\n",
      "Epoch 97 Train RMSE Loss: 117.7029, Val RMSE Loss: 130.9138\n",
      "Epoch 98 Train RMSE Loss: 117.0210, Val RMSE Loss: 119.8416\n",
      "Epoch 99 Train RMSE Loss: 117.6650, Val RMSE Loss: 98.4220\n",
      "Epoch 100 Train RMSE Loss: 115.9246, Val RMSE Loss: 124.7567\n",
      "\n",
      "Total training time on 100 epochs: 25084.42596435547\n",
      "Train Losses: \n",
      "[290.93979684784557, 287.1600553944441, 285.9006956681614, 283.6914879397835, 283.0418477959553, 281.97717517949144, 279.940602886172, 276.78344656499985, 274.1742428859655, 268.341890305007, 261.39277209634974, 252.392263281524, 243.69425605425633, 235.36487531261605, 227.71767756867646, 221.45869933615668, 217.3325855133061, 212.7872322590664, 210.37766176413913, 207.4407571283597, 205.1047086242649, 203.61237777640122, 200.30733115822622, 198.65768135456713, 195.9896084362445, 194.17834140051824, 192.1067629352456, 191.20574428523605, 186.78217825415254, 184.8541031292554, 184.85453189425783, 181.83876658105106, 178.83445082844716, 178.12204689215935, 176.0591822881557, 174.10022046888176, 172.9297418727839, 170.93134187399139, 168.34692284371167, 166.43567944981893, 165.44992165549894, 162.55345567554122, 162.45136260111423, 160.62920333603324, 159.06159791024882, 156.97493775436453, 155.4780963639887, 154.97719224084707, 152.9189610309225, 151.01365537728645, 149.55103174219133, 149.65138572175817, 146.2298043032916, 145.08720341265743, 144.77039449389352, 142.5202469171023, 143.0842451258783, 141.19240621678125, 140.81169243986534, 139.30163009488706, 138.87111913415134, 137.48798101810794, 138.05020576459512, 136.13626287856616, 133.76178592382334, 135.73107993277, 134.84424582638877, 134.91890703706915, 132.16735602684145, 132.24631019341004, 130.62880668742974, 131.2021430422217, 129.14847799334711, 129.61139684852074, 128.16870917340395, 127.73980484189657, 127.24597500088571, 124.68436946417022, 125.5077646331824, 126.45398853769613, 126.27042574934048, 125.79316206682151, 126.15688563642746, 125.0931981453878, 124.31220740344797, 123.94804101966102, 123.62565467745145, 122.26818906044794, 122.16372537463121, 121.15691606251248, 120.56654068681594, 120.64004766271945, 120.80484549764665, 119.24406347438008, 118.81088860748663, 120.89060441727122, 117.70285540297152, 117.02097729437303, 117.6650218739851, 115.9245513837059]\n",
      "####################################################################################################\n",
      "Val losses: \n",
      "[91.41994622281733, 136.7365640180165, 138.1516792319564, 93.66545601964039, 135.59195936101585, 139.46909397364035, 139.9445390110791, 97.60488023907604, 129.37294590005817, 136.70863176844395, 131.3499721566277, 131.39992460251594, 134.93223668232048, 120.96671461572342, 71.03040876784085, 116.43908146487077, 117.53681115947916, 119.8917368480961, 120.86549303761832, 119.40289830663005, 113.82901750894094, 120.12914727045131, 117.2755307379305, 111.32095645780127, 116.51187217370544, 117.78807148924822, 107.31088669752289, 121.74419663401825, 115.76656452075228, 115.725985777524, 120.71288239283302, 119.56932859567421, 96.33664804143527, 76.08644147236572, 114.52660024241503, 119.28305446508121, 106.79085329013692, 116.6589810991582, 112.03251067107988, 88.88692466261031, 118.30915317547353, 114.67549111173996, 124.11644076890308, 112.17860068724532, 116.08375367040358, 119.32302277703343, 137.8303101533645, 100.40137709930825, 123.73768976807602, 117.92054271640396, 124.19826187170706, 115.88235427250467, 119.08930759557614, 146.26427798226015, 129.2814407328784, 119.28739883368995, 118.97820486299949, 135.55211017447832, 130.0541443970684, 141.56720121884695, 117.13971682070918, 120.1027402388914, 137.15335654985125, 153.699140668662, 97.40433695887693, 135.0184686531491, 131.9315988584378, 124.93463623062311, 155.9514648055606, 141.8387386548651, 106.78030629978808, 113.24751167847877, 137.24478845481826, 152.83604752111003, 117.90623835268627, 149.56333413630745, 140.76799240433485, 121.53434588215755, 144.12656943665218, 132.8650148933766, 127.09788621634925, 141.39561488839732, 114.72845988173962, 151.35446176344828, 126.01187792301782, 190.12647543189837, 137.01956928016762, 118.49073465557866, 125.21475546177281, 168.54392654670548, 126.51089611546311, 130.09389450208164, 154.11480674233272, 128.9256462680137, 136.6879384923808, 144.37374695073115, 130.91378742809914, 119.84160561763063, 98.42201701152638, 124.75670170031198]\n"
     ]
    }
   ],
   "source": [
    "model = xLSTM(stack_config='ss', hidden_size=256)\n",
    "model.to('cuda')\n",
    "trainer = Trainer(model, 100, train_loader, val_loader)\n",
    "start = time.time()\n",
    "train_losses, val_losses = trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "print(f'\\nTotal training time on 100 epochs: {end-start}')  \n",
    "print(\"Train Losses: \")\n",
    "print(train_losses)\n",
    "print(\"#\"*100)\n",
    "print(\"Val losses: \")\n",
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ms layer 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train RMSE Loss: 290.7972, Val RMSE Loss: 137.1976\n",
      "Epoch 2 Train RMSE Loss: 288.2851, Val RMSE Loss: 135.1946\n",
      "Epoch 3 Train RMSE Loss: 287.3735, Val RMSE Loss: 95.9218\n",
      "Epoch 4 Train RMSE Loss: 286.8527, Val RMSE Loss: 134.6585\n",
      "Epoch 5 Train RMSE Loss: 285.6837, Val RMSE Loss: 133.9326\n",
      "Epoch 6 Train RMSE Loss: 284.1383, Val RMSE Loss: 136.5771\n",
      "Epoch 7 Train RMSE Loss: 282.7144, Val RMSE Loss: 130.2358\n",
      "Epoch 8 Train RMSE Loss: 281.3967, Val RMSE Loss: 132.6759\n",
      "Epoch 9 Train RMSE Loss: 280.2014, Val RMSE Loss: 128.9540\n",
      "Epoch 10 Train RMSE Loss: 279.2505, Val RMSE Loss: 138.1473\n",
      "Epoch 11 Train RMSE Loss: 277.7063, Val RMSE Loss: 135.8556\n",
      "Epoch 12 Train RMSE Loss: 275.3076, Val RMSE Loss: 140.7158\n",
      "Epoch 13 Train RMSE Loss: 274.2897, Val RMSE Loss: 104.4488\n",
      "Epoch 14 Train RMSE Loss: 272.3030, Val RMSE Loss: 104.2478\n",
      "Epoch 15 Train RMSE Loss: 270.1256, Val RMSE Loss: 135.0117\n",
      "Epoch 16 Train RMSE Loss: 268.5810, Val RMSE Loss: 141.5973\n",
      "Epoch 17 Train RMSE Loss: 266.0271, Val RMSE Loss: 137.7000\n",
      "Epoch 18 Train RMSE Loss: 265.2998, Val RMSE Loss: 135.9946\n",
      "Epoch 19 Train RMSE Loss: 263.5312, Val RMSE Loss: 134.8882\n",
      "Epoch 20 Train RMSE Loss: 262.2752, Val RMSE Loss: 100.1610\n",
      "Epoch 21 Train RMSE Loss: 261.1307, Val RMSE Loss: 134.0884\n",
      "Epoch 22 Train RMSE Loss: 259.4071, Val RMSE Loss: 134.1445\n",
      "Epoch 23 Train RMSE Loss: 258.8011, Val RMSE Loss: 96.2307\n",
      "Epoch 24 Train RMSE Loss: 256.9993, Val RMSE Loss: 123.6525\n",
      "Epoch 25 Train RMSE Loss: 256.5425, Val RMSE Loss: 143.6367\n",
      "Epoch 26 Train RMSE Loss: 254.9498, Val RMSE Loss: 127.4512\n",
      "Epoch 27 Train RMSE Loss: 253.6602, Val RMSE Loss: 137.9065\n",
      "Epoch 28 Train RMSE Loss: 251.8349, Val RMSE Loss: 146.8018\n",
      "Epoch 29 Train RMSE Loss: 250.6193, Val RMSE Loss: 134.4835\n",
      "Epoch 30 Train RMSE Loss: 249.4233, Val RMSE Loss: 144.2569\n",
      "Epoch 31 Train RMSE Loss: 248.0063, Val RMSE Loss: 146.6712\n",
      "Epoch 32 Train RMSE Loss: 245.6068, Val RMSE Loss: 134.8670\n",
      "Epoch 33 Train RMSE Loss: 244.4311, Val RMSE Loss: 134.7258\n",
      "Epoch 34 Train RMSE Loss: 242.9681, Val RMSE Loss: 140.6985\n",
      "Epoch 35 Train RMSE Loss: 240.9723, Val RMSE Loss: 131.6294\n",
      "Epoch 36 Train RMSE Loss: 239.2770, Val RMSE Loss: 138.4357\n",
      "Epoch 37 Train RMSE Loss: 238.2587, Val RMSE Loss: 136.3856\n",
      "Epoch 38 Train RMSE Loss: 236.7845, Val RMSE Loss: 130.0181\n",
      "Epoch 39 Train RMSE Loss: 233.3538, Val RMSE Loss: 133.2340\n",
      "Epoch 40 Train RMSE Loss: 232.7667, Val RMSE Loss: 138.9738\n",
      "Epoch 41 Train RMSE Loss: 230.5891, Val RMSE Loss: 130.1244\n",
      "Epoch 42 Train RMSE Loss: 228.6851, Val RMSE Loss: 107.6230\n",
      "Epoch 43 Train RMSE Loss: 227.4314, Val RMSE Loss: 131.2363\n",
      "Epoch 44 Train RMSE Loss: 225.0214, Val RMSE Loss: 128.1872\n",
      "Epoch 45 Train RMSE Loss: 222.8461, Val RMSE Loss: 129.5010\n",
      "Epoch 46 Train RMSE Loss: 221.0023, Val RMSE Loss: 137.2161\n",
      "Epoch 47 Train RMSE Loss: 220.0536, Val RMSE Loss: 126.5756\n",
      "Epoch 48 Train RMSE Loss: 217.1296, Val RMSE Loss: 134.5657\n",
      "Epoch 49 Train RMSE Loss: 215.2475, Val RMSE Loss: 134.9851\n",
      "Epoch 50 Train RMSE Loss: 213.2602, Val RMSE Loss: 138.9683\n",
      "Epoch 51 Train RMSE Loss: 212.4838, Val RMSE Loss: 123.7179\n",
      "Epoch 52 Train RMSE Loss: 210.4501, Val RMSE Loss: 130.3821\n",
      "Epoch 53 Train RMSE Loss: 209.5182, Val RMSE Loss: 99.8261\n",
      "Epoch 54 Train RMSE Loss: 206.7873, Val RMSE Loss: 129.6068\n",
      "Epoch 55 Train RMSE Loss: 205.5100, Val RMSE Loss: 123.4992\n",
      "Epoch 56 Train RMSE Loss: 204.2035, Val RMSE Loss: 127.1902\n",
      "Epoch 57 Train RMSE Loss: 204.7918, Val RMSE Loss: 84.5691\n",
      "Epoch 58 Train RMSE Loss: 203.0768, Val RMSE Loss: 131.7655\n",
      "Epoch 59 Train RMSE Loss: 202.2941, Val RMSE Loss: 128.6999\n",
      "Epoch 60 Train RMSE Loss: 200.9374, Val RMSE Loss: 125.6848\n",
      "Epoch 61 Train RMSE Loss: 200.5075, Val RMSE Loss: 132.0096\n",
      "Epoch 62 Train RMSE Loss: 199.2135, Val RMSE Loss: 163.0539\n",
      "Epoch 63 Train RMSE Loss: 198.9765, Val RMSE Loss: 123.1426\n",
      "Epoch 64 Train RMSE Loss: 197.8957, Val RMSE Loss: 127.4760\n",
      "Epoch 65 Train RMSE Loss: 197.2170, Val RMSE Loss: 137.4334\n",
      "Epoch 66 Train RMSE Loss: 195.6272, Val RMSE Loss: 128.9888\n",
      "Epoch 67 Train RMSE Loss: 195.9050, Val RMSE Loss: 127.2547\n",
      "Epoch 68 Train RMSE Loss: 196.8203, Val RMSE Loss: 124.1228\n",
      "Epoch 69 Train RMSE Loss: 195.0993, Val RMSE Loss: 133.6121\n",
      "Epoch 70 Train RMSE Loss: 195.0375, Val RMSE Loss: 134.3764\n",
      "Epoch 71 Train RMSE Loss: 194.6635, Val RMSE Loss: 103.2326\n",
      "Epoch 72 Train RMSE Loss: 193.2308, Val RMSE Loss: 124.9398\n",
      "Epoch 73 Train RMSE Loss: 191.9437, Val RMSE Loss: 133.2302\n",
      "Epoch 74 Train RMSE Loss: 192.6807, Val RMSE Loss: 127.2108\n",
      "Epoch 75 Train RMSE Loss: 191.6793, Val RMSE Loss: 127.5305\n",
      "Epoch 76 Train RMSE Loss: 191.4773, Val RMSE Loss: 118.0825\n",
      "Epoch 77 Train RMSE Loss: 190.3792, Val RMSE Loss: 138.7245\n",
      "Epoch 78 Train RMSE Loss: 190.7377, Val RMSE Loss: 126.5558\n",
      "Epoch 79 Train RMSE Loss: 190.5793, Val RMSE Loss: 129.8196\n",
      "Epoch 80 Train RMSE Loss: 189.6941, Val RMSE Loss: 128.1993\n",
      "Epoch 81 Train RMSE Loss: 188.3396, Val RMSE Loss: 103.6519\n",
      "Epoch 82 Train RMSE Loss: 188.6194, Val RMSE Loss: 128.2780\n",
      "Epoch 83 Train RMSE Loss: 187.8503, Val RMSE Loss: 125.2520\n",
      "Epoch 84 Train RMSE Loss: 187.6109, Val RMSE Loss: 132.7213\n",
      "Epoch 85 Train RMSE Loss: 187.0946, Val RMSE Loss: 107.2539\n",
      "Epoch 86 Train RMSE Loss: 187.4611, Val RMSE Loss: 129.1180\n",
      "Epoch 87 Train RMSE Loss: 187.2475, Val RMSE Loss: 99.0382\n",
      "Epoch 88 Train RMSE Loss: 186.0686, Val RMSE Loss: 96.1338\n",
      "Epoch 89 Train RMSE Loss: 184.1823, Val RMSE Loss: 84.5704\n",
      "Epoch 90 Train RMSE Loss: 184.3177, Val RMSE Loss: 130.9451\n",
      "Epoch 91 Train RMSE Loss: 184.8272, Val RMSE Loss: 124.0052\n",
      "Epoch 92 Train RMSE Loss: 184.0816, Val RMSE Loss: 134.3769\n",
      "Epoch 93 Train RMSE Loss: 185.4206, Val RMSE Loss: 120.6456\n",
      "Epoch 94 Train RMSE Loss: 183.3684, Val RMSE Loss: 132.6428\n",
      "Epoch 95 Train RMSE Loss: 183.6362, Val RMSE Loss: 134.5193\n",
      "Epoch 96 Train RMSE Loss: 183.4538, Val RMSE Loss: 138.7537\n",
      "Epoch 97 Train RMSE Loss: 182.4043, Val RMSE Loss: 136.2424\n",
      "Epoch 98 Train RMSE Loss: 182.4438, Val RMSE Loss: 132.1524\n",
      "Epoch 99 Train RMSE Loss: 182.9705, Val RMSE Loss: 133.1614\n",
      "Epoch 100 Train RMSE Loss: 180.8063, Val RMSE Loss: 83.9212\n",
      "\n",
      "Total training time on 100 epochs: 34899.52364778519\n",
      "Train Losses: \n",
      "[290.79717462651803, 288.2851407179671, 287.3735374934112, 286.8527444897604, 285.6837391632527, 284.1383143382329, 282.71441763887293, 281.39672141621503, 280.2013515572211, 279.25050859932486, 277.7062864095059, 275.3075516999759, 274.28967712223016, 272.30297574728104, 270.1255637684927, 268.5810499088171, 266.0270810602617, 265.29984071883285, 263.5311866340359, 262.2752474311397, 261.13070032192053, 259.40708781594685, 258.80105656402463, 256.9992600099904, 256.54248677537026, 254.94978021534118, 253.66017992795724, 251.83486698201634, 250.61933067597775, 249.4232964929614, 248.00630427375535, 245.60680495464672, 244.431143625809, 242.96806497811295, 240.9723019946411, 239.27702508225377, 238.2586712837408, 236.78448803895697, 233.35378195332194, 232.76674544059955, 230.5890630094059, 228.6851035483671, 227.43140661535682, 225.02143044795747, 222.84610624416246, 221.00234560476977, 220.05364877770683, 217.12963840806944, 215.24753056406954, 213.26022548116487, 212.48379226817494, 210.4500762335133, 209.5181622059758, 206.78732887390714, 205.5099932817265, 204.20353217582704, 204.791794867428, 203.0767610984003, 202.29412374578501, 200.93736105678198, 200.50746657885216, 199.21354723310333, 198.97647409100338, 197.8957141632744, 197.21700237954016, 195.62718499370945, 195.90498846142802, 196.82027680332146, 195.09933179685194, 195.0374792035611, 194.6635013853326, 193.2307684813409, 191.94372551624238, 192.68074490391484, 191.6793378235186, 191.4772685170807, 190.37923178737967, 190.73771183519483, 190.57931074901555, 189.6940526008038, 188.3396187882869, 188.6194412390106, 187.850314209776, 187.61091608865186, 187.0945589254531, 187.46114032072688, 187.24753286886684, 186.06864056583177, 184.18228375085832, 184.31767331029218, 184.82724854438047, 184.08164412790978, 185.42064776118008, 183.36837493302912, 183.63616481254294, 183.4537678253768, 182.4042523504983, 182.44376031167099, 182.97051862487066, 180.8063262929035]\n",
      "####################################################################################################\n",
      "Val losses: \n",
      "[137.19757045896895, 135.19460709329118, 95.92184573016331, 134.6584578426275, 133.9326175542085, 136.57712217913502, 130.23581892195955, 132.67587365897342, 128.9540242910603, 138.14734070600355, 135.85562189324372, 140.71575109923506, 104.44877281508924, 104.2477552872093, 135.01174807823438, 141.597275150612, 137.70004712604722, 135.9946431619643, 134.88819245171396, 100.16099954612137, 134.08842443106806, 134.1444633987809, 96.230660851366, 123.65251250037845, 143.63665368299974, 127.45115294385826, 137.90648191403568, 146.80181770550604, 134.48352403660712, 144.2569318193399, 146.6711588316115, 134.86703114129557, 134.72582111458811, 140.69847299361274, 131.62938750255157, 138.43565311620753, 136.38558244774032, 130.01807453977793, 133.23398744373094, 138.97382135661715, 130.12439586195012, 107.62300886024704, 131.236261836805, 128.1872314569347, 129.5010208619873, 137.21608916943057, 126.57563999111423, 134.56571132373395, 134.98510660034407, 138.9682554592617, 123.71789602903681, 130.38212225122882, 99.82613792238985, 129.6067708071708, 123.4992260619095, 127.1901556464154, 84.56910254619443, 131.76545546502788, 128.69989970691628, 125.68484902490857, 132.0095969670624, 163.05394165927115, 123.14256051406815, 127.47603031811143, 137.43336009425394, 128.98883528781184, 127.25474074850315, 124.12282066314397, 133.61206563043342, 134.3764155539539, 103.23257883067366, 124.9397579445165, 133.23023459469044, 127.21075757679347, 127.53052254141947, 118.08252739659322, 138.7245287360778, 126.55579264634136, 129.81964840994314, 128.19925582598339, 103.6518608224335, 128.27804597856223, 125.25203400138608, 132.72132072357243, 107.25393747780166, 129.1179823616394, 99.03816550976225, 96.13381056470926, 84.57037637122299, 130.94509692311172, 124.00522430243363, 134.37692267883207, 120.64558853197477, 132.64283372233353, 134.51934924859555, 138.75369065333678, 136.24242695051782, 132.15238987771136, 133.16139238119447, 83.9212240770232]\n"
     ]
    }
   ],
   "source": [
    "model = xLSTM(stack_config='ms', hidden_size=256)\n",
    "model.to('cuda')\n",
    "trainer = Trainer(model, 100, train_loader, val_loader)\n",
    "start = time.time()\n",
    "train_losses, val_losses = trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "print(f'\\nTotal training time on 100 epochs: {end-start}')  \n",
    "print(\"Train Losses: \")\n",
    "print(train_losses)\n",
    "print(\"#\"*100)\n",
    "print(\"Val losses: \")\n",
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valley_fever",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
