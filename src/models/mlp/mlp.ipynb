{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17724, 6935])\n",
      "Epoch [100/1500], Loss: 287.5648, Validation Loss: 128.6548\n",
      "Epoch [200/1500], Loss: 284.1812, Validation Loss: 128.4311\n",
      "Epoch [300/1500], Loss: 277.7788, Validation Loss: 131.1986\n",
      "Epoch [400/1500], Loss: 263.9069, Validation Loss: 130.8083\n",
      "Epoch [500/1500], Loss: 233.2045, Validation Loss: 133.5549\n",
      "Epoch [600/1500], Loss: 181.4070, Validation Loss: 134.0724\n",
      "Epoch [700/1500], Loss: 129.1598, Validation Loss: 131.5045\n",
      "Epoch [800/1500], Loss: 95.1695, Validation Loss: 124.3192\n",
      "Epoch [900/1500], Loss: 75.0208, Validation Loss: 120.4086\n",
      "Epoch [1000/1500], Loss: 63.8049, Validation Loss: 118.8566\n",
      "Epoch [1100/1500], Loss: 57.0620, Validation Loss: 116.6111\n",
      "Epoch [1200/1500], Loss: 52.3870, Validation Loss: 115.3359\n",
      "Epoch [1300/1500], Loss: 48.6233, Validation Loss: 114.4863\n",
      "Epoch [1400/1500], Loss: 45.2549, Validation Loss: 114.8049\n",
      "Epoch [1500/1500], Loss: 42.0685, Validation Loss: 115.1563\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  \n",
    "        self.fc2 = nn.Linear(hidden_size, int(hidden_size / 2))\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  \n",
    "        self.fc3 = nn.Linear(int(hidden_size / 2), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # out = self.dropout1(out)  \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        # out = self.dropout2(out)  \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "input_size = 365*19\n",
    "hidden_size = 1024\n",
    "output_size = 1  \n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "model.to('cuda')\n",
    "train_dataset = torch.load('../../../data/cleaned/train.pt')\n",
    "train_x, train_y = train_dataset.tensors  \n",
    "\n",
    "val_dataset = torch.load('../../../data/cleaned/val.pt')\n",
    "test_x, test_y = val_dataset.tensors  \n",
    "\n",
    "train_x = train_x.reshape(-1, 365*19)\n",
    "test_x = test_x.reshape(-1, 365*19)\n",
    "\n",
    "train_x = train_x.to('cuda')\n",
    "test_x = test_x.to('cuda')\n",
    "train_y = train_y.to('cuda')\n",
    "test_y = test_y.to('cuda')\n",
    "\n",
    "print(train_x.shape)\n",
    "criterion = nn.MSELoss()  \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "num_epochs = 1500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    outputs = model(train_x)\n",
    "    loss = criterion(outputs, train_y.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(test_x)\n",
    "            val_loss = criterion(val_outputs, test_y.unsqueeze(1))\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {np.sqrt(loss.item()):.4f}, Validation Loss: {np.sqrt(val_loss.item()):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17724, 6935])\n",
      "Epoch [1/100], Loss: 300.0679, Validation Loss: 134.2102\n",
      "Epoch [2/100], Loss: 299.9646, Validation Loss: 134.0870\n",
      "Epoch [3/100], Loss: 299.8718, Validation Loss: 133.9628\n",
      "Epoch [4/100], Loss: 299.7782, Validation Loss: 133.8347\n",
      "Epoch [5/100], Loss: 299.6811, Validation Loss: 133.6985\n",
      "Epoch [6/100], Loss: 299.5776, Validation Loss: 133.5538\n",
      "Epoch [7/100], Loss: 299.4672, Validation Loss: 133.4006\n",
      "Epoch [8/100], Loss: 299.3503, Validation Loss: 133.2380\n",
      "Epoch [9/100], Loss: 299.2259, Validation Loss: 133.0667\n",
      "Epoch [10/100], Loss: 299.0945, Validation Loss: 132.8873\n",
      "Epoch [11/100], Loss: 298.9564, Validation Loss: 132.6996\n",
      "Epoch [12/100], Loss: 298.8113, Validation Loss: 132.5031\n",
      "Epoch [13/100], Loss: 298.6587, Validation Loss: 132.2974\n",
      "Epoch [14/100], Loss: 298.4982, Validation Loss: 132.0829\n",
      "Epoch [15/100], Loss: 298.3300, Validation Loss: 131.8597\n",
      "Epoch [16/100], Loss: 298.1538, Validation Loss: 131.6278\n",
      "Epoch [17/100], Loss: 297.9697, Validation Loss: 131.3879\n",
      "Epoch [18/100], Loss: 297.7778, Validation Loss: 131.1404\n",
      "Epoch [19/100], Loss: 297.5784, Validation Loss: 130.8861\n",
      "Epoch [20/100], Loss: 297.3719, Validation Loss: 130.6253\n",
      "Epoch [21/100], Loss: 297.1583, Validation Loss: 130.3583\n",
      "Epoch [22/100], Loss: 296.9375, Validation Loss: 130.0857\n",
      "Epoch [23/100], Loss: 296.7097, Validation Loss: 129.8080\n",
      "Epoch [24/100], Loss: 296.4750, Validation Loss: 129.5260\n",
      "Epoch [25/100], Loss: 296.2336, Validation Loss: 129.2404\n",
      "Epoch [26/100], Loss: 295.9857, Validation Loss: 128.9522\n",
      "Epoch [27/100], Loss: 295.7317, Validation Loss: 128.6624\n",
      "Epoch [28/100], Loss: 295.4720, Validation Loss: 128.3721\n",
      "Epoch [29/100], Loss: 295.2069, Validation Loss: 128.0825\n",
      "Epoch [30/100], Loss: 294.9368, Validation Loss: 127.7948\n",
      "Epoch [31/100], Loss: 294.6621, Validation Loss: 127.5104\n",
      "Epoch [32/100], Loss: 294.3837, Validation Loss: 127.2310\n",
      "Epoch [33/100], Loss: 294.1018, Validation Loss: 126.9580\n",
      "Epoch [34/100], Loss: 293.8174, Validation Loss: 126.6933\n",
      "Epoch [35/100], Loss: 293.5310, Validation Loss: 126.4387\n",
      "Epoch [36/100], Loss: 293.2435, Validation Loss: 126.1960\n",
      "Epoch [37/100], Loss: 292.9558, Validation Loss: 125.9673\n",
      "Epoch [38/100], Loss: 292.6687, Validation Loss: 125.7545\n",
      "Epoch [39/100], Loss: 292.3831, Validation Loss: 125.5597\n",
      "Epoch [40/100], Loss: 292.1000, Validation Loss: 125.3849\n",
      "Epoch [41/100], Loss: 291.8205, Validation Loss: 125.2322\n",
      "Epoch [42/100], Loss: 291.5457, Validation Loss: 125.1037\n",
      "Epoch [43/100], Loss: 291.2765, Validation Loss: 125.0012\n",
      "Epoch [44/100], Loss: 291.0141, Validation Loss: 124.9264\n",
      "Epoch [45/100], Loss: 290.7596, Validation Loss: 124.8811\n",
      "Epoch [46/100], Loss: 290.5139, Validation Loss: 124.8666\n",
      "Epoch [47/100], Loss: 290.2783, Validation Loss: 124.8839\n",
      "Epoch [48/100], Loss: 290.0535, Validation Loss: 124.9338\n",
      "Epoch [49/100], Loss: 289.8406, Validation Loss: 125.0173\n",
      "Epoch [50/100], Loss: 289.6404, Validation Loss: 125.1338\n",
      "Epoch [51/100], Loss: 289.4536, Validation Loss: 125.2829\n",
      "Epoch [52/100], Loss: 289.2807, Validation Loss: 125.4634\n",
      "Epoch [53/100], Loss: 289.1221, Validation Loss: 125.6744\n",
      "Epoch [54/100], Loss: 288.9783, Validation Loss: 125.9138\n",
      "Epoch [55/100], Loss: 288.8492, Validation Loss: 126.1784\n",
      "Epoch [56/100], Loss: 288.7348, Validation Loss: 126.4651\n",
      "Epoch [57/100], Loss: 288.6346, Validation Loss: 126.7705\n",
      "Epoch [58/100], Loss: 288.5482, Validation Loss: 127.0904\n",
      "Epoch [59/100], Loss: 288.4748, Validation Loss: 127.4203\n",
      "Epoch [60/100], Loss: 288.4136, Validation Loss: 127.7544\n",
      "Epoch [61/100], Loss: 288.3632, Validation Loss: 128.0879\n",
      "Epoch [62/100], Loss: 288.3226, Validation Loss: 128.4160\n",
      "Epoch [63/100], Loss: 288.2904, Validation Loss: 128.7334\n",
      "Epoch [64/100], Loss: 288.2650, Validation Loss: 129.0355\n",
      "Epoch [65/100], Loss: 288.2451, Validation Loss: 129.3176\n",
      "Epoch [66/100], Loss: 288.2292, Validation Loss: 129.5758\n",
      "Epoch [67/100], Loss: 288.2159, Validation Loss: 129.8067\n",
      "Epoch [68/100], Loss: 288.2041, Validation Loss: 130.0080\n",
      "Epoch [69/100], Loss: 288.1926, Validation Loss: 130.1777\n",
      "Epoch [70/100], Loss: 288.1806, Validation Loss: 130.3149\n",
      "Epoch [71/100], Loss: 288.1673, Validation Loss: 130.4194\n",
      "Epoch [72/100], Loss: 288.1522, Validation Loss: 130.4919\n",
      "Epoch [73/100], Loss: 288.1350, Validation Loss: 130.5337\n",
      "Epoch [74/100], Loss: 288.1156, Validation Loss: 130.5466\n",
      "Epoch [75/100], Loss: 288.0940, Validation Loss: 130.5329\n",
      "Epoch [76/100], Loss: 288.0704, Validation Loss: 130.4953\n",
      "Epoch [77/100], Loss: 288.0450, Validation Loss: 130.4368\n",
      "Epoch [78/100], Loss: 288.0181, Validation Loss: 130.3604\n",
      "Epoch [79/100], Loss: 287.9900, Validation Loss: 130.2692\n",
      "Epoch [80/100], Loss: 287.9612, Validation Loss: 130.1661\n",
      "Epoch [81/100], Loss: 287.9319, Validation Loss: 130.0542\n",
      "Epoch [82/100], Loss: 287.9025, Validation Loss: 129.9361\n",
      "Epoch [83/100], Loss: 287.8732, Validation Loss: 129.8144\n",
      "Epoch [84/100], Loss: 287.8443, Validation Loss: 129.6912\n",
      "Epoch [85/100], Loss: 287.8158, Validation Loss: 129.5686\n",
      "Epoch [86/100], Loss: 287.7880, Validation Loss: 129.4486\n",
      "Epoch [87/100], Loss: 287.7609, Validation Loss: 129.3323\n",
      "Epoch [88/100], Loss: 287.7345, Validation Loss: 129.2211\n",
      "Epoch [89/100], Loss: 287.7087, Validation Loss: 129.1159\n",
      "Epoch [90/100], Loss: 287.6835, Validation Loss: 129.0174\n",
      "Epoch [91/100], Loss: 287.6590, Validation Loss: 128.9263\n",
      "Epoch [92/100], Loss: 287.6348, Validation Loss: 128.8428\n",
      "Epoch [93/100], Loss: 287.6110, Validation Loss: 128.7673\n",
      "Epoch [94/100], Loss: 287.5875, Validation Loss: 128.6999\n",
      "Epoch [95/100], Loss: 287.5641, Validation Loss: 128.6406\n",
      "Epoch [96/100], Loss: 287.5408, Validation Loss: 128.5893\n",
      "Epoch [97/100], Loss: 287.5175, Validation Loss: 128.5457\n",
      "Epoch [98/100], Loss: 287.4940, Validation Loss: 128.5098\n",
      "Epoch [99/100], Loss: 287.4705, Validation Loss: 128.4811\n",
      "Epoch [100/100], Loss: 287.4468, Validation Loss: 128.4593\n"
     ]
    }
   ],
   "source": [
    "# for creatinig losses over 100 epochs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  \n",
    "        self.fc2 = nn.Linear(hidden_size, int(hidden_size / 2))\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  \n",
    "        self.fc3 = nn.Linear(int(hidden_size / 2), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # out = self.dropout1(out)  \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        # out = self.dropout2(out)  \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "input_size = 365*19\n",
    "hidden_size = 1024\n",
    "output_size = 1  \n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "model.to('cuda')\n",
    "train_dataset = torch.load('../../../data/cleaned/train.pt')\n",
    "train_x, train_y = train_dataset.tensors  \n",
    "\n",
    "val_dataset = torch.load('../../../data/cleaned/val.pt')\n",
    "test_x, test_y = val_dataset.tensors  \n",
    "\n",
    "train_x = train_x.reshape(-1, 365*19)\n",
    "test_x = test_x.reshape(-1, 365*19)\n",
    "\n",
    "train_x = train_x.to('cuda')\n",
    "test_x = test_x.to('cuda')\n",
    "train_y = train_y.to('cuda')\n",
    "test_y = test_y.to('cuda')\n",
    "\n",
    "print(train_x.shape)\n",
    "criterion = nn.MSELoss()  \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    outputs = model(train_x)\n",
    "    loss = criterion(outputs, train_y.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    train_losses.append(np.sqrt(loss.item()))\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(test_x)\n",
    "            val_loss = criterion(val_outputs, test_y.unsqueeze(1))\n",
    "            val_losses.append(np.sqrt(val_loss.item()))\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {np.sqrt(loss.item()):.4f}, Validation Loss: {np.sqrt(val_loss.item()):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300.067882944843, 299.964633331998, 299.87175644348366, 299.7782383487834, 299.68110655328275, 299.5775932458901, 299.46724831356767, 299.35027298718137, 299.22587622062366, 299.094466682351, 298.95638791469236, 298.81126463873477, 298.65869420627956, 298.4982464353853, 298.3300002723494, 298.1537983994167, 297.96965292207193, 297.7777983455113, 297.57841666105423, 297.3718868058983, 297.1582727478742, 296.9374934224373, 296.7096908174723, 296.4749936756893, 296.2335834058995, 295.9857207282135, 295.73169322039195, 295.47197413629607, 295.2068527956287, 294.93675169432515, 294.66214763691653, 294.38365167753454, 294.10182388502795, 293.81737259307863, 293.5310087963451, 293.24353946421394, 292.9558147144378, 292.66866130916713, 292.3830829365817, 292.1000203269421, 291.8205382808071, 291.54566537679824, 291.27648752946055, 291.0141077980585, 290.75956562080637, 290.5139438348872, 290.2782743549713, 290.0535241554565, 289.840635091079, 289.64040205054266, 289.453580043502, 289.28065416650315, 289.1221087187903, 288.978305305952, 288.8492253754543, 288.73475338535195, 288.6346225117493, 288.5482384931504, 288.47482829962826, 288.41357566002335, 288.36320142747064, 288.3226003541866, 288.2903538535412, 288.2649876849424, 288.2450671737506, 288.2291704312733, 288.21592925270454, 288.20411061433526, 288.19263034904276, 288.1805803224777, 288.16726913374464, 288.1521814727419, 288.1349916922622, 288.1156045105853, 288.09401948409135, 288.0703852793619, 288.0449726171245, 288.0180658396275, 287.9900036242578, 287.96117902418723, 287.9319038418633, 287.9024900031259, 287.87319528309683, 287.8442639909644, 287.8158319568262, 287.7880214889077, 287.760927789893, 287.7344696069625, 287.70868784675235, 287.68354195626137, 287.65895062556285, 287.63479177161446, 287.6110111765542, 287.58747310600995, 287.56410969990674, 287.5407987355881, 287.51748588129385, 287.49404885145015, 287.470514792387, 287.44676137504143]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134.21015324622425, 134.08696600760643, 133.96279071770266, 133.83474972657513, 133.6985444245748, 133.55381347513068, 133.40058037261306, 133.2380022154903, 133.06673460852267, 132.88733920834972, 132.69959171235232, 132.5030807424869, 132.29738340666455, 132.08291518729817, 131.85970734790442, 131.6278341337804, 131.38790374307675, 131.14040811521443, 130.8860877585926, 130.62526166241736, 130.35828032771835, 130.0856839019959, 129.80801629233073, 129.5259988380711, 129.24043424466663, 128.95219257054143, 128.66240952585957, 128.37213969549623, 128.0824929488804, 127.79478407171985, 127.51043921724605, 127.23097767323608, 126.95803163595046, 126.69334285983616, 126.43867785536393, 126.19600098284613, 125.96725765054981, 125.75448472991927, 125.55968451298371, 125.3849151737361, 125.23223348917602, 125.10373039751852, 125.00121093163457, 124.92644710988942, 124.88105278123659, 124.86657723095881, 124.88385228883676, 124.93383405076865, 125.01734254694426, 125.13376436587768, 125.28285185431405, 125.4633560484395, 125.67436452370666, 125.91381213329021, 126.17841024096595, 126.46506666244834, 126.7705003761719, 127.09043439860059, 127.42032973465223, 127.75437630101953, 128.08791383996774, 128.415965347966, 128.73344098611284, 129.03546932432957, 129.31755971251545, 129.5757500460638, 129.80674487724048, 130.0079700080922, 130.1776580533311, 130.31485309626066, 130.41941058082574, 130.49191785317586, 130.53369266926833, 130.54657484256336, 130.5328921670703, 130.4953303762246, 130.4368261841724, 130.36040786028556, 130.2691699544639, 130.16613482872572, 130.05420294294606, 129.9361096005456, 129.81435061271154, 129.6911897065487, 129.56864282350494, 129.4485524614895, 129.33229216585858, 129.22111167733004, 129.11589391899048, 129.01741797379532, 128.9262522360167, 128.8427911982273, 128.7672925280329, 128.6998992326723, 128.64057850266377, 128.5892579713601, 128.545749698794, 128.5098032495965, 128.48111333723335, 128.4593275248045]\n"
     ]
    }
   ],
   "source": [
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valley_fever",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
