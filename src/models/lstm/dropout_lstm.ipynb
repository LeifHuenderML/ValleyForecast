{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lstm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the datasets\n",
    "train = torch.load('../../../data/cleaned/train.pt')\n",
    "val = torch.load('../../../data/cleaned/val.pt')\n",
    "test = torch.load('../../../data/cleaned/test.pt')\n",
    "\n",
    "#make them into dataloaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "---------\n",
      "Train Loss RMSE: 291.95592139538746, Validation Loss RMSE: 127.82627969219755\n",
      "\n",
      "Epoch: 2/100\n",
      "---------\n",
      "Train Loss RMSE: 291.31423696823003, Validation Loss RMSE: 134.0121010144829\n",
      "\n",
      "Epoch: 3/100\n",
      "---------\n",
      "Train Loss RMSE: 290.99284851632984, Validation Loss RMSE: 136.40975091361406\n",
      "\n",
      "Epoch: 4/100\n",
      "---------\n",
      "Train Loss RMSE: 291.3466537257931, Validation Loss RMSE: 134.8970165146792\n",
      "\n",
      "Epoch: 5/100\n",
      "---------\n",
      "Train Loss RMSE: 290.8603739321774, Validation Loss RMSE: 135.31954750539913\n",
      "\n",
      "Epoch: 6/100\n",
      "---------\n",
      "Train Loss RMSE: 291.47137697302765, Validation Loss RMSE: 88.39413877516155\n",
      "\n",
      "Epoch: 7/100\n",
      "---------\n",
      "Train Loss RMSE: 291.1162781906085, Validation Loss RMSE: 134.8795927120368\n",
      "\n",
      "Epoch: 8/100\n",
      "---------\n",
      "Train Loss RMSE: 290.69199490335114, Validation Loss RMSE: 132.73795433973726\n",
      "\n",
      "Epoch: 9/100\n",
      "---------\n",
      "Train Loss RMSE: 290.7292537917628, Validation Loss RMSE: 136.2018224125284\n",
      "\n",
      "Epoch: 10/100\n",
      "---------\n",
      "Train Loss RMSE: 290.8518202109491, Validation Loss RMSE: 133.23592966753685\n",
      "\n",
      "Epoch: 11/100\n",
      "---------\n",
      "Train Loss RMSE: 290.35763628697026, Validation Loss RMSE: 134.6001236684356\n",
      "\n",
      "Epoch: 12/100\n",
      "---------\n",
      "Train Loss RMSE: 290.2204385193486, Validation Loss RMSE: 134.51306860301716\n",
      "\n",
      "Epoch: 13/100\n",
      "---------\n",
      "Train Loss RMSE: 290.2001880858624, Validation Loss RMSE: 126.03803242974261\n",
      "\n",
      "Epoch: 14/100\n",
      "---------\n",
      "Train Loss RMSE: 289.8878333979113, Validation Loss RMSE: 135.01566297303017\n",
      "\n",
      "Epoch: 15/100\n",
      "---------\n",
      "Train Loss RMSE: 290.2848859862484, Validation Loss RMSE: 133.1569691844284\n",
      "\n",
      "Epoch: 16/100\n",
      "---------\n",
      "Train Loss RMSE: 290.50867587716874, Validation Loss RMSE: 133.3720345342742\n",
      "\n",
      "Epoch: 17/100\n",
      "---------\n",
      "Train Loss RMSE: 290.3815405137614, Validation Loss RMSE: 133.1641976354795\n",
      "\n",
      "Epoch: 18/100\n",
      "---------\n",
      "Train Loss RMSE: 290.2970339402705, Validation Loss RMSE: 130.1817074209026\n",
      "\n",
      "Epoch: 19/100\n",
      "---------\n",
      "Train Loss RMSE: 290.2735318896166, Validation Loss RMSE: 133.12934102207606\n",
      "\n",
      "Epoch: 20/100\n",
      "---------\n",
      "Train Loss RMSE: 290.3477564219378, Validation Loss RMSE: 128.63887290338522\n",
      "\n",
      "Epoch: 21/100\n",
      "---------\n",
      "Train Loss RMSE: 290.23009219093285, Validation Loss RMSE: 132.90350864697697\n",
      "\n",
      "Epoch: 22/100\n",
      "---------\n",
      "Train Loss RMSE: 290.5182283040046, Validation Loss RMSE: 133.50048231148082\n",
      "\n",
      "Epoch: 23/100\n",
      "---------\n",
      "Train Loss RMSE: 290.19193180097517, Validation Loss RMSE: 87.63528340925524\n",
      "\n",
      "Epoch: 24/100\n",
      "---------\n",
      "Train Loss RMSE: 290.1727283250831, Validation Loss RMSE: 134.21530941417038\n",
      "\n",
      "Epoch: 25/100\n",
      "---------\n",
      "Train Loss RMSE: 290.3493646979594, Validation Loss RMSE: 129.3313218055373\n",
      "\n",
      "Epoch: 26/100\n",
      "---------\n",
      "Train Loss RMSE: 290.6675694354095, Validation Loss RMSE: 131.91726064317345\n",
      "\n",
      "Epoch: 27/100\n",
      "---------\n",
      "Train Loss RMSE: 290.282686631022, Validation Loss RMSE: 133.54893186805305\n",
      "\n",
      "Epoch: 28/100\n",
      "---------\n",
      "Train Loss RMSE: 290.19240959498654, Validation Loss RMSE: 133.4444905610172\n",
      "\n",
      "Epoch: 29/100\n",
      "---------\n",
      "Train Loss RMSE: 290.2121367206606, Validation Loss RMSE: 133.81871597064483\n",
      "\n",
      "Epoch: 30/100\n",
      "---------\n",
      "Train Loss RMSE: 290.53935590927426, Validation Loss RMSE: 128.85456773823495\n",
      "\n",
      "Epoch: 31/100\n",
      "---------\n",
      "Train Loss RMSE: 290.265717071267, Validation Loss RMSE: 131.77322332936325\n",
      "\n",
      "Epoch: 32/100\n",
      "---------\n",
      "Train Loss RMSE: 290.3601523486303, Validation Loss RMSE: 133.61740576988643\n",
      "\n",
      "Epoch: 33/100\n",
      "---------\n",
      "Train Loss RMSE: 290.40361117131323, Validation Loss RMSE: 134.49881940825387\n",
      "\n",
      "Epoch: 34/100\n",
      "---------\n",
      "Train Loss RMSE: 290.3816365777606, Validation Loss RMSE: 132.9845843898076\n",
      "\n",
      "Epoch: 35/100\n",
      "---------\n",
      "Train Loss RMSE: 289.9151632548142, Validation Loss RMSE: 132.8152565073422\n",
      "\n",
      "Epoch: 36/100\n",
      "---------\n",
      "Train Loss RMSE: 290.3609505221283, Validation Loss RMSE: 132.620769491185\n",
      "\n",
      "Epoch: 37/100\n",
      "---------\n",
      "Train Loss RMSE: 290.361283715317, Validation Loss RMSE: 76.37062563303488\n",
      "\n",
      "Epoch: 38/100\n",
      "---------\n",
      "Train Loss RMSE: 290.4438556512494, Validation Loss RMSE: 132.98887884798458\n",
      "\n",
      "Epoch: 39/100\n",
      "---------\n",
      "Train Loss RMSE: 290.1693287036663, Validation Loss RMSE: 133.6834884294524\n",
      "\n",
      "Epoch: 40/100\n",
      "---------\n",
      "Train Loss RMSE: 290.35897633908223, Validation Loss RMSE: 128.30951881238457\n",
      "\n",
      "Epoch: 41/100\n",
      "---------\n",
      "Train Loss RMSE: 290.38259933061397, Validation Loss RMSE: 133.0992156289294\n",
      "\n",
      "Epoch: 42/100\n",
      "---------\n",
      "Train Loss RMSE: 290.06627337433787, Validation Loss RMSE: 75.781027616369\n",
      "\n",
      "Epoch: 43/100\n",
      "---------\n",
      "Train Loss RMSE: 290.3123817838407, Validation Loss RMSE: 78.67856339838164\n",
      "\n",
      "Epoch: 44/100\n",
      "---------\n",
      "Train Loss RMSE: 290.4703369667552, Validation Loss RMSE: 134.01723499706551\n",
      "\n",
      "Epoch: 45/100\n",
      "---------\n",
      "Train Loss RMSE: 290.05462853356266, Validation Loss RMSE: 131.10025461655354\n",
      "\n",
      "Epoch: 46/100\n",
      "---------\n",
      "Train Loss RMSE: 290.43857508736943, Validation Loss RMSE: 133.16792175185316\n",
      "\n",
      "Epoch: 47/100\n",
      "---------\n",
      "Train Loss RMSE: 290.2415405031386, Validation Loss RMSE: 129.42723654417492\n",
      "\n",
      "Epoch: 48/100\n",
      "---------\n",
      "Train Loss RMSE: 290.57392757114, Validation Loss RMSE: 129.59238618586326\n",
      "\n",
      "Epoch: 49/100\n",
      "---------\n",
      "Train Loss RMSE: 290.70779180254425, Validation Loss RMSE: 132.18427645793705\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate,)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m lstm\u001b[38;5;241m.\u001b[39mTrainer(model, train_loader, val_loader, loss_fn, optimizer,)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/Current/ValleyForecast/src/models/lstm/lstm.py:211\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m#train and validate a batch\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_one_epoch()\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m#calculate the RMSE\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research/Current/ValleyForecast/src/models/lstm/lstm.py:187\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(output, y)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 187\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    189\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/valley_fever/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/valley_fever/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/valley_fever/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = lstm.LSTM(hidden_size=2560, num_layers=3, dropout=0.8)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,)\n",
    "trainer = lstm.Trainer(model, train_loader, val_loader, loss_fn, optimizer,)\n",
    "trainer.train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valley_fever",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
