## Monday

- rewrote the build dataset notebook to be a script
- add data augmentation to increase the size of the dataset
- added documentation
- improved the readme
- created a new weekly work log under [research/weekly_work_log/06_10_to_06_16.md](06_10_to_06_16.md) 
- wrote the baseline regressor and began training it
- wrote the section for the paper covering the performance and model selection for the baseline regressor
- split the dataset into train validation and test datasets
- build a base reg to run on the train and test datasets
- built a lstm to train on the train val and test datasets
- write a summray of the work i am on now, sent that, my github and contact info to mary

## Tuesday
- read Head, J. R., Sondermeyer-Cooksey, G., Heaney, A. K., Yu, A. T., Jones, I., Bhattachan, A., Campo, S. K., Wagner, R., Mgbara, W., Phillips, S., Keeney, N., Taylor, J., Eisen, E., Lettenmaier, D. P., Hubbard, A., Okin, G. S., Vugia, D. J., Jain, S., & Remais, J. v. (2022). Effects of precipitation, heat, and drought on incidence and expansion of coccidioidomycosis in western USA: a longitudinal surveillance study. The Lancet Planetary Health, 6(10), e793–e803. https://doi.org/10.1016/S2542-5196(22)00202-9
- read Mulliken, J. S., Hampshire, K. N., Rappold, A. G., Fung, M., Babik, J. M., & Doernberg, S. B. (2023). Risk of systemic fungal infections after exposure to wildfires: a population-based, retrospective study in California. The Lancet Planetary Health, 7(5), e381–e386. https://doi.org/10.1016/S2542-5196(23)00046-3
- read Childs, M. L., Li, J., Wen, J., Heft-Neal, S., Driscoll, A., Wang, S., Gould, C. F., Qiu, M., Burney, J., & Burke, M. (2022). Daily Local-Level Estimates of Ambient Wildfire Smoke PM2.5for the Contiguous US. Environmental Science and Technology, 56(19), 13607–13621. https://doi.org/10.1021/acs.est.2c02934
- change the prep for the dataset so that it considered the context of the whole year
- changed it to improve its abiliyt for generalizing well by augmenting  the data much more using a  logaritmic function to create varied noise distribuction over the original data
- retrained the base reg on that and got a rmse of 383
- built a mlp to see if the data is the issure or if the lstm model is, i think it is the lstm becaus the mlp on thoe data got a rmse of 161
- worked on the lstm trying out different variation to improve it
- wrote a custom grid search algorithm to explore a large search space when the time comes
- created another df of the yearly avearages for each feature and plotted it with a parralel coordinates graph

## Wednesday
- got grid search running but still want to add more functionalities
- found a good amount of augmentation for the dataset whre the  models seem to train best
- added lots of documentation and comments to the code forthe lstm, trainer, and the gridsearch.
- implemented a early stopping function
- further improved grid search
- Added even more documentation
- created a new script to run a broad grid search on 160 models of varying size
- started training those models 
- made updates to the research paper
- add a feature to the gridsearch that will log all the tested paraameters into a csv, i want to store each parameter, train time, model size, train rmse, validation rmse, test rmse
- run the gridsearch searching for the best set of hyperparameters with the regular lstm

## Thursday
- started off by doing a deep dive over the xlstm paper again
-looked for more information out there on it
- reviewed open sourced code on it https://github.com/AI-Guru/xlstm-resources?tab=readme-ov-file
- started building ithe xlstm based on 
- watched https://www.youtube.com/watch?v=L5RshXUwdFA on creating a custom layer in pytorch
- read over a bunch of sources on building lstms
- i think i figured it out to the point where i can get a slstm built by the end of the week now
- today was not a write a bunch of code kind of day but more of a think hard about designing this necxt phase. 
- got a best performant lstm to do an 86% improvement over the baseline regressor


## Friday 
- 2 hr meeting in the morning 

## Saturday 

## Sunday

## TODO
- run the lstm that had the best performance and look at all the metrics for it
- finish building the slstm 
- test the slstm
- start on the mlstm
- try to find efficiencies with the slstm and mlstm
- document
- look into the valley fever grant
- add data to a usb for mary
- build the xlstm
- get the xlstm training 
- experiment with different architectures with the xlstm
- run the xlstm through the grid search too