## Monday
- finished writing the fast pitch
- added more documentation to the scltm block
- added more docutmenation to the mlstm block
- added more documentation to the xlstm block
- finalized the fast pitch shematics to better go wit the speech i wrote
- wrote the training loop funciton for the xlstm
- started debugging the code for the xlstm
- wrote a Lego class that stacks the blocks for the lstm in a very easy to define way
- wrote a grid search algoritm that will search hidden layer parameter sizes and save the best result
- wrote the first draft to the abstract of the paper
- made the template for the poster



## Tuesday
- started working through debugging the code
- i was running into a lot of issues some were over my head so i started looking around at other programers solutions
- this is the least messy xlstm repo i found https://github.com/muditbhargava66/PyxLSTM
- there are issuse with it though so i am rewriting it to be better and fix any issues with testing on our own data
- the rewrite took most of the day but it was a great learning experience seeing how someone else interpretted the xlstm paper line by line. 
- rewrote the slstm i dont like it though i followed the way the other guy did it and it is not correct, he is missing normalizers and a lot of stuff is just kind of off, i am going to try and rewrite the slstm as a cell and then a layer then a block like my original implmentation and just use his code asa reference but adhere to the paper more.

## Wednesday
- i spent all day trying to get the slstm to work i wrote a brand new one with a new cell, layer and embedded into its own model i got it to begint training but it is not learning i tried so many different things and nothing seems to work, tomorrow i am going to swap it for the forward pass of the regular custom lstm and see if that doies anything.

## Thursday
- practiced the fast pitch speech several times
- continued debuggin the slstm and stll struggling to get ti to train
## Friday 
- made progress on slstm training, i decided to make slow adaptations to the customlstmcell and see how things improve, training time is very slow for this so i am also working on other tasks like improving my abstract.
- check where the weights for the hidden state is initialized and try movind it back some levels

## Saturday 
- wrote most of the paper
- read more research papers about cm
- read more papers about deep neural networks
- lots of editing
- fixed my poster dimensions
- edited the fast pitch more
- practiced presenting it more
- edited the paper more
- started learning latex for writing the paper


## Sunday

## TODO

- try to find efficiencies with the slstm and mlstm
- get the xlstm training 
- experiment with different architecture variants with the xlstm
- run the xlstm through the grid search too
- continue writing the paper and compile results
- update the graphs for the poster, and create them into one figure for the poster do this in canva
- create a loss graph of the accuracy over time that plots the lstm xltsm and the mlp on it
- crete the math forward passes for the paper
- edit the paper based on the improvements reccomended
- read through more research papres for wrtining the paper
- update the poster to johns reccomendations
- get the new poster printed



