## Monday
- run the lstm that had the best performance and look at all the metrics for it
- since i am builting the xlstm fromm complete scratch i wanted to rewrite all the tools that i would normally use from numpy or torch so that i have a complete picture of what is going on inside the model
- wrote a sigmoid function
- wrote a tanh function
- wrote a matrix multiplication functino tath takes tensors nm and mp and returns np tensor
- reread a few blog posts on lstms, it seems that each person has their own way of doing a lstm
- after seeing the runtime differences i will cotninue to us eth eoriginal np and tencor builtins because they are orders of magnitude faster by utilizing cuda
- finally got the custom LSTM model to run, this is great news because then it will allow me to easily edit and improve upon it so that i can incorporate the sLSTM and mLSTM architectures better
- began editing the LSTM model to be more optimized
- wrote the research grant

## Tuesday
- edited the valley fever grant
- fixed up the custom lstm architecture some more
- ran a trarining run on the lstm that i built from scratch, yeild the same rmse as the pytorch one
- extremely slow though
- built the slstm class
- added a bunch of documentation to the slstm class
- wrote the forward pass of the sltm cell
- wrote a model to test a slstm layer
- ran into a handful of issuse with the shape of data going in and out of the model



## Wednesday

## Thursday

## Friday 

## Saturday 

## Sunday

## TODO
- update the grant proposal
- write the backward pass for the slstm
- test the slstm
- start on the mlstm
- try to find efficiencies with the slstm and mlstm
- document
- build the xlstm
- get the xlstm training 
- experiment with different architectures with the xlstm
- run the xlstm through the grid search too