## Monday
- run the lstm that had the best performance and look at all the metrics for it
- since i am builting the xlstm fromm complete scratch i wanted to rewrite all the tools that i would normally use from numpy or torch so that i have a complete picture of what is going on inside the model
- wrote a sigmoid function
- wrote a tanh function
- wrote a matrix multiplication functino tath takes tensors nm and mp and returns np tensor
- reread a few blog posts on lstms, it seems that each person has their own way of doing a lstm
- after seeing the runtime differences i will cotninue to us eth eoriginal np and tencor builtins because they are orders of magnitude faster by utilizing cuda
- finally got the custom LSTM model to run, this is great news because then it will allow me to easily edit and improve upon it so that i can incorporate the sLSTM and mLSTM architectures better
- began editing the LSTM model to be more optimized
- wrote the research grant

## Tuesday
- edited the valley fever grant
- fixed up the custom lstm architecture some more
- ran a trarining run on the lstm that i built from scratch, yeild the same rmse as the pytorch one
- extremely slow though
- built the slstm class
- added a bunch of documentation to the slstm class
- wrote the forward pass of the sltm cell
- wrote a model to test a slstm layer
- ran into a handful of issuse with the shape of data going in and out of the model

## Wednesday
National Holiday 

## Thursday
Wrote a test that compared the training of the first custom lstm to the improved one, the first had an issue with training but hte custom one yielded similar if not better results from the pytorch builtin one
Wrote the class for the mlstm
aded documentation based on the paper for the mlstm
wrote the forward pass for the mlstm

## Friday 

## Saturday 

## Sunday

## TODO
- update the grant proposal
- write the backward pass for the slstm
- test the slstm
- start on the mlstm
- try to find efficiencies with the slstm and mlstm
- document
- build the xlstm
- get the xlstm training 
- experiment with different architectures with the xlstm
- run the xlstm through the grid search too