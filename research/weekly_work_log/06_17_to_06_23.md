## Monday
- run the lstm that had the best performance and look at all the metrics for it
- since i am builting the xlstm fromm complete scratch i wanted to rewrite all the tools that i would normally use from numpy or torch so that i have a complete picture of what is going on inside the model
- wrote a sigmoid function
- wrote a tanh function
- wrote a matrix multiplication functino tath takes tensors nm and mp and returns np tensor
- reread a few blog posts on lstms, it seems that each person has their own way of doing a lstm
- after seeing the runtime differences i will cotninue to us eth eoriginal np and tencor builtins because they are orders of magnitude faster by utilizing cuda
- finally got the custom LSTM model to run, this is great news because then it will allow me to easily edit and improve upon it so that i can incorporate the sLSTM and mLSTM architectures better
- began editing the LSTM model to be more optimized
- wrote the research grant

## Tuesday
- edited the valley fever grant
- fixed up the custom lstm architecture some more
- ran a trarining run on the lstm that i built from scratch, yeild the same rmse as the pytorch one
- extremely slow though
- built the slstm class
- added a bunch of documentation to the slstm class
- wrote the forward pass of the sltm cell
- wrote a model to test a slstm layer
- ran into a handful of issuse with the shape of data going in and out of the model

## Wednesday
National Holiday 

## Thursday
- Wrote a test that compared the training of the first custom lstm to the improved one, the first had an issue with training but hte - - custom one yielded similar if not better results from the pytorch builtin one
- Wrote the class for the mlstm
- aded documentation based on the paper for the mlstm
- wrote the forward pass for the mlstm
- created the block class for slstm
- created the block class for mlstm
- created the forward pass for slstm block class
- created the forward pass for mlstm block class
- i know there are some bugs in there i decided it would be best to try and assemble it all and then spend a day working on bug fixes
- wrote the xlstm model alternating between mlstm and slstm blocks for a total of 4 blocks then passing through a 3 layer mlp
- worte the forward pass for the xsltm
- worked at nic for a few hours helping kevin with labelling images for his dataset

## Friday 
- 2 hr friday meeting
- submitted the valley fever grant application
- learned that i have a fast pitch in 2 weeks that i need to have ready to present so i am shifting from the xlstm for the moment and will focut on getting everything for the fast pitch ready then i can pivot backt to the xlstm
- i need to have a visuals for the fast pitch so i went and learnde how do use plotlys choroplath feature to create graphs of counties in california
- buiilt a few test graphs, the feature works great if you have a nice distribution of data but kern county had 6X more than even the next highest number so on real case data it didnt loook very good
- i spent a while trying to make custom colorscales that go through the specrum for hte low numbers and then rapidly rise from there
- this didnt work well so i looked  around and say that you can overlay 2 choroplaths as traces i tried this for like 2 hours to no success

## Saturday 
- i had an ideat to just use a discrete range of numbers for coloring the graph, this version worked the best it shows the spread of intensity very well witout being consumed by the outlier
- now that i have a good system going for graphing the choroplaths i need to make a dataset that alings the predictions from the baseline regressor to the counnty that it is predicted in, since the base reg just outputs the mean for each of the entries i can just swap the data from each of the endries in the origialt dataset
-wrote an algoritm that scales colors so that i can have a nice variance of red for the mapping
-had to rewrite the data prep to create a new dataset for testing the trained model on, this dataset i left he county in the output so that i can easily slice out the county when i run inferences
- created two dictionaaries one to contain the outputs and one to contain the target values these are aligned so that they are matche with the county then each county is sumed to represent the number of cases over the spand of the 20ish years
- had to do a bunch of remanipluation and labelling with the data to get it to line up and merge niccely with the county geojston df that i have
- got the maps made they turned out very nice and will work great for the fast pitch schematics i will need 
- i wrote most of the fast pitch too
- tweaked the graphs further to get it to be exactly how i want



## Sunday

## TODO

- try to find efficiencies with the slstm and mlstm
- document
- get the xlstm training 
- experiment with different architecture variants with the xlstm
- run the xlstm through the grid search too
- continue writing the paper and compile results
- create a graph of the 48 counties in california with a heatmap representing the level of affected areas
- added more docutmenation to the mlstm block
- added more documentation to the scltm block
- added more documentation to the xlstm block
- worked on getting some schematics made for the fast pitch presentation