## Monday
- run the lstm that had the best performance and look at all the metrics for it
- since i am builting the xlstm fromm complete scratch i wanted to rewrite all the tools that i would normally use from numpy or torch so that i have a complete picture of what is going on inside the model
- wrote a sigmoid function
- wrote a tanh function
- wrote a matrix multiplication functino tath takes tensors nm and mp and returns np tensor
- reread a few blog posts on lstms, it seems that each person has their own way of doing a lstm
- after seeing the runtime differences i will cotninue to us eth eoriginal np and tencor builtins because they are orders of magnitude faster by utilizing cuda


## Tuesday

## Wednesday

## Thursday

## Friday 

## Saturday 

## Sunday

## TODO
- finish building the slstm 
- test the slstm
- start on the mlstm
- try to find efficiencies with the slstm and mlstm
- document
- look into the valley fever grant
- build the xlstm
- get the xlstm training 
- experiment with different architectures with the xlstm
- run the xlstm through the grid search too