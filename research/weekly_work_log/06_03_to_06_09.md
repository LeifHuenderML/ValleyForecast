## Monday
- made the first draft for the ispa poster took most of the day
- purchased the rest of the travel items like the hotel
- sent out reciepts to everyone who needed them

## Tuesday
- started really working on figuring oout what i will be doing for the next stint of this project
- i did a literature review today and would add stuff to the paper i am working on for this project
- i read this article on the new xlstm architecture https://medium.com/@zergtant/deep-dive-into-xlstm-the-evolution-of-lstm-architecture-and-pytorch-code-implementation-d901a14bbcec
- i think that currently the xlstm architecture may yield to be valuable to have in the context of what we are working the residual block that prevend  gradienct explosion although are designed for being abhle to make the model deeper thy will probably yield valuue for us too.
- i realized that part of my data is incorrectly structured so i am going to fix that

## Wednesday

## Thursday
- A lot of the work for today was dedicated to getting work done for my previous project on similarity scoring microclimates
- downloaded the dataset for the california valley fever 
- started making different visualizations of that data 
- cleaned the dataset 
- i found a handfull of counties where there is a zero count for cases i am putting a pin in it to wait ans see how accurate the records are before moving forward to see as to why this is the case i created a new dataset where i removed any county from the df where there was more than 60 of the data marked as a zero, my reasoning is that if they are marked at zero it is more likely due to improper data collection than there actually being no cases but i do want to see it this reasoning is correct. 
- create the structure for the valleyforecast repo

## Friday 

- was in a meeting for INBRE for 2 hours
- helped Storm learn how to get going with vs code for 1 hour
- purchased the registration for the event
- submitted my paper to the conference
- purchased the flights
- purchased the weather dataset
- created a readme for dataset cleaning
- created a readme for dataset visualization
- removed la from the dataset see readme for details on why


## Saturday 

- retrieved all the data from open weather
- began cleaning the dataset 
- wrote a research outline and prospectives for the next 8 weeks 
- create an outline for the next paper


## Sunday

- finished cleaning the weather dataset to get it to the point where i woul have data that would be worth visualizing
- began making visualizations to the weather data
- updated documentation
- read [The Rise of Valley Fever Prevalence and Cost Burden of Coccidioidomycosis Infection in California](<../../papers/The Rise of Valley Fever Prevalence and Cost Burden of Coccidioidomycosis Infection in California.pdf>)
- started working on the paper

## TODO
- get what size mary is for her hoodie - medium
- collect papers and look to answer research questions
- with the lstm model i want to try training a few batches one on the season leading up to the cases
- try training on unaggergated data
- try trainning on aggregated data
- train using dropout and without dropout
- add data augmentation to it to see if it changet the models performance the reason i want to do this is because the xlstm architecture worskt best on large datasets



- read this article first
https://medium.com/@zergtant/deep-dive-into-xlstm-the-evolution-of-lstm-architecture-and-pytorch-code-implementation-d901a14bbcec
- after reading and taking notes evaluate if xlstm is worth trying out


https://github.com/AI-Guru/xlstm-resources?tab=readme-ov-file

