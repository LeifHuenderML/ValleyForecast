## Monday
- made the first draft for the ispa poster took most of the day
- purchased the rest of the travel items like the hotel
- sent out reciepts to everyone who needed them

## Tuesday
- started really working on figuring oout what i will be doing for the next stint of this project
- i did a literature review today and would add stuff to the paper i am working on for this project
- i read this article on the new xlstm architecture https://medium.com/@zergtant/deep-dive-into-xlstm-the-evolution-of-lstm-architecture-and-pytorch-code-implementation-d901a14bbcec
- i think that currently the xlstm architecture may yield to be valuable to have in the context of what we are working the residual block that prevend  gradienct explosion although are designed for being abhle to make the model deeper thy will probably yield valuue for us too.
- i realized that part of my data is incorrectly structured so i am going to fix that
- it took a lot of work but i got the dataset corrsectly shaped for the lstm training
- i added a 4x increase in data by augmenting it with some noise
- improved the documentation of the dataset creation file
- improved the documentation of the data readme log
- created a new directory to save all graphic visualizations to
- finished editing the poster 

## Wednesday
- read through [papers/Understanding LSTM – a tutorial into Long Short-Term Memory Recurrent Neural Networks.pdf](<../../papers/Understanding LSTM – a tutorial into Long Short-Term Memory Recurrent Neural Networks.pdf>)
- read trough [(<../../papers/Investigating the Relationship Between Climate and Valley Fever (Coccidioidomycosis).pdf>)](<../../papers/Investigating the Relationship Between Climate and Valley Fever (Coccidioidomycosis).pdf>)
- read through [papers/The Rise of Valley Fever Prevalence and Cost Burden of Coccidioidomycosis Infection in California.pdf](<../../papers/The Rise of Valley Fever Prevalence and Cost Burden of Coccidioidomycosis Infection in California.pdf>)
- read trough [papers/xLSTM- Extended Long Short-Term Memory.pdf](<../../papers/xLSTM- Extended Long Short-Term Memory.pdf>)
-watched a video on xLSTM to understand it even further https://www.youtube.com/watch?v=0OaEv1a5jUM
- read [papers/A_Deep_Learning_Approach_to_Predict_Weather_Data_Using_Cascaded_LSTM_Network.pdf](../../papers/A_Deep_Learning_Approach_to_Predict_Weather_Data_Using_Cascaded_LSTM_Network.pdf)
- made edits to my paper
- wrote the entire introduction
- started on methodologies, i wrot the data sourcing and data preperation
- built a regressor
- build a lstm 


## Thursday
- yesterday when i built the lstm i noticetd that the training was only yielding a 10% imrovement over the dummy reggessor, so i built a mlp to test if the data is the issue
- luckily the data is not the issue, although i did run through all the data cleaning and prep code and i did narorw down the features further to beiong only 13 features per sequence
- this is because i think that the llatitude and longitude would not contribute weull to generalizing the model, we did see some improvements from there 
- the mlp performed very good with a validation rmse of like 80 which is a improvmend of 70% over the baseline. This is more what i was expectving to see
- today i will comb through and really work on improving the lstm architecture to establish a strong model that yields better improvements 
- i rewrote the whole lstm trying diffeerent size models to see if there was anything that I was doing wrong. I could not find anything. I think the issue lies in the data
- I rewrote all of the datacleaning from start to finish and nothing was yielding better results
- I think that there is some type of mismatch or corruption of the data happening


## Friday 
- today i am moving all the data cleaning code to archive 
- i rewrote the entire data cleaning process from scratch 
- i think i found the bug, i am not 100% sure i ran out of hours to work in my 40hours so i didnt get to thouroughly test my code yet
- i will on monday test it with a mlp lstm and base reg, increase the size of data through augmentation, create the train test val sets, i also need to document all the changes mate to the repo



## Saturday 

## Sunday


## TODO
- send the poster off to get printed
- send mary the reciept for the poster
- get what size mary is for her hoodie - medium
- train using dropout and without dropout


https://github.com/AI-Guru/xlstm-resources?tab=readme-ov-file

